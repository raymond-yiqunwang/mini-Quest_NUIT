## slurm.conf file generated by configurator.html.
## Put this file on all nodes of your cluster.
## See the slurm.conf man page for more information.
#
##ControlMachine=qslurm01 # This is the hostname of the scheduler 
ControlMachine=qnode0
ControlAddr=129.105.37.62
##ControlAddr=172.20.5.11 # This is the IP address of the scheduler (optional if you don't want it to use /etc/hosts)
##BackupController=qslurm02 # This is the hostname of the backup scheduler
##BackupAddr=172.20.5.12 # This is the IP address  of the backup scheduler (optional if you don't want it to use /etc/hosts)
# 
AuthType=auth/munge # How communications between nodes are secured. Can use none or munge, should always use munge.
CheckpointType=checkpoint/none # What integrated checkpoint tool to use. Could use ompi for OpenMPI version 1.3+
CryptoType=crypto/munge # Whether to use munge or ssl for cryptographic signature.
DisableRootJobs=NO # Whether to prevent root from running jobs 
EnforcePartLimits=ALL # Prevents jobs with invalid parameters from being submitted (rather than blocked)
##Epilog=/hpc/slurm/snapshot.epilog # Script to run to clean up user processes at the end of sbatch job. Don't use the default one as it will kill all of a user's jobs on a node when one ends. Better to use cgroups and PAM for control.
##Prolog=/hpc/slurm/snapshot.prolog # Path of a program for slurmd to execute whenever asked to run a job step from a new job allocation.
##SrunEpilog= # Script to run to clean up user processes at the end of srun job. See note for Epilog.
##SrunProlog=/hpc/slurm/srun.prolog # Prologue script to run for srun jobs.
##EpilogSlurmctld= # Epilogue script to run as slurm user on job completion/failure.
FirstJobId=1 # The first ID to assign to a job.
##MaxJobId=999999 # The highest possible ID to assign to a job before it rolls over to assign the FirstJobId again.
MaxJobID=99
##GresTypes=gpu # Defines types of generic resources for nodes. Here we've defined gpus as a consumable resource. Also configured in gres.conf.
GroupUpdateForce=1 # 1 Checks which user group membership periodically for access to partitions. 0 only checks when /etc/group has been updated. This may be helpful if using the database to control access to queues.
##X11Parameters=local_xauthority # TODO: Restore this once restored native X11# Prevents X11 locks on Xauthority by using a temp file instead of /home/$USER/.Xauthority
GroupUpdateTime=600 # How often to check which users are members of groups allowed to use a partition. Time is in seconds.
##JobCheckpointDir=/hpc/slurm/checkpoint # Location to store job checkpoint information. Should be writable by SlurmUser but not by regular users.
##JobCredentialPrivateKey= # Path to private key for authentication if not using munge.
##JobCredentialPublicCertificate= # Path to public key for authentication if not using munge.
##JobFileAppend=0 # Determines what to do if error/output files already exist for a job. 0 is truncate old info, 1 is append new info.
JobRequeue=1 # Controls whether batch jobs will be requeued. Can be overridden by user submit settings --requeue or --no-requeue.
JobSubmitPlugins=lua # Comma delimited list of job submission plugins. 
##KillOnBadExit=0 # Terminate job step immediately if any tasks have a non-0 exit code.
##LaunchType=launch/slurm # Special launch parameters for all-in-one clusters. Otherwise use slurm.
##Licenses=mdcsw:600 # Specifies licenses that can be requested by a job. Jobs can still use licenses that aren't explicitly requested.
##MailProg=/usr/bin/mail # Path to mail program used to send notification emails.
##MaxJobCount=300000 # Maximum number of jobs before the scheduler begins rejecting submissions. Per documentation, performance can suffer if more than a few hundred thousand jobs are being run. 
MaxJobCount=30 # Maximum number of jobs before the scheduler begins rejecting submissions. Per documentation, performance can suffer if more than a few hundred thousand jobs are being run. 
MaxArraySize=5000 # Maximum array size
MaxStepCount=40000 # Maximum number of steps any job can initiate. This is intended to prevent bad scripts from overwhelming nodes and the scheduler. Default is 40,000.
##MaxTasksPerNode=512 # Maximum number of tasks Slurm will allow a job step to spawn on a single node. Default is 512, maximum is 65533.
MaxTasksPerNode=24 # Maximum number of tasks Slurm will allow a job step to spawn on a single node. Default is 512, maximum is 65533.
MpiDefault=none # Default MPI type to use, can be overridden by job. Options are openmpi, pmi2, pmix, and none (which is the default, and works for many other versions of MPI).
##MpiParams=ports=#-# # Port range used by older versions of OpenMPI. If using a modern version of OpenMPI, leave this blank for better performance. 
##PluginDir= # Colon-separated list of directories where to look for Slurm plugins. Default is /usr/local/lib/slurm.
##PlugStackConfig= # Location of Slurm stackable Plugin Architecture for Node job (K)control (SPANK). Default location is plugstack.conf in same directory as slurm.conf.
##PrivateData= # Comma delimeted list of information to hide from users. By default users can see everything. Root and SlurmUser can always see everything.
ProctrackType=proctrack/cgroup # Similar to TaskPlugin, how slurm manages processes on a node. Default is using cgroups.
PrologFlags=contain,alloc#,X11 # TODO: Restore X11 once using native capability # Needed for PAM to prevent users from sshing without a job
##PrologSlurmctld= # Path for a program for slurmctld to execute before granting a new job allocation.
##PropagatePrioProcess=0 # Determines the scheduling priority (nice value) of user spawned tasks. 0 inherit from slurmd, 1 inherit from srun or sbatch but no higher than slurmd, and 2 is the same as 1 but one priority higher than slurmd. Default is 0.
PropagateResourceLimits=NONE # Which limits (from ulimit) to inherit from the login node and apply to the job. Default value is ALL.
##PropagateResourceLimitsExcept= # Which limits not to inherit from the login node and apply to the job.
##RebootProgram= # Program to be executed on each compute node to reboot it. Invoked on idle nodes after scontrol reboot_nodes or a job is submitted with --reboot option. Will wait for node to become idle before executing.
ReturnToService=2 # Controls when a down node is returned to service. 0: manually, 1: recovery after being non-responsive, 2: recovery after any circumstance, including bad memory, etc.
##SallocDefaultCommand= # Default command to execute when running salloc.
SlurmctldPidFile=/slurmctld_data/slurmctld.pid # Must be writable by root, preferable also writable by SlurmUser. File must be available for both primary and backup schedulers.
SlurmctldPort=6817 # The port slurmctld communicates over. 6817 is the default.
SlurmdPidFile=/slurmd_data/slurmd.pid # Location of slurmd pid file for nodes.
SlurmdPort=6818 # The port slurmd communicates over. 6818 is the default.
SlurmdSpoolDir=/slurmd_data # Path where slurmd daemon state and batch job script information are written.
SlurmUser=slurm # User that slurmctld executes as. Recommended to use a service account rather than root.
SlurmdUser=root # User that the slurmd daemon executes as for communication via munge as well as executing UnkillableStepProgram. Usually root.
StateSaveLocation=/slurmctld_data # Path where the scheduler saves its state. Should be readable and writable by both the primary and backup scheduler. SlurmUser should have access.
SwitchType=switch/none # This is used for special types of switches, like Cray and IBM all-in-one clusters. For Ethernet or Infiniband, use none.
##TaskEpilog= # Path to program to execute at completion of each job task.
TaskPlugin=task/cgroup # How slurm manages resources on a node. cgroup will use cgroups to constrain jobs to the resources they've requested.
##TaskPluginParam= # Comma separated list of optional parameters for the TaskPlugin.
##TaskProlog= # Path to program to run at the beginning of each job task.
TopologyPlugin=topology/none # Identifies plugin for determinine network topology and minimizing network contention. If tree, use topology.conf.
TmpFS=/tmp # Where jobs should point to for temporary storage. Default value is /tmp.
##TrackWCKey=no 
##TreeWidth= 
##UnkillableStepProgram=  # Program to execute if a task isn't killed after sending it a SIGKILL and waiting UnlikkableStepTimeout.
UnkillableStepTimeout=60 # Amount of time in seconds to wait for a task to be killed after sending it a SIGKILL command before calling UnkillableStepProgram and marking the node down.
##UsePAM=0 
PreemptMode=REQUEUE # What to do with pre-empted jobs. Default is OFF.
PreemptType=preempt/partition_prio # How pre-emption should function. Can either use qos or priorityTier based pre-emption.
# 
# 
# TIMERS 
##BatchStartTimeout=10 
##CompleteWait=0 
##EpilogMsgTime=2000 
##GetEnvTimeout=2 
##HealthCheckInterval=1800 # How often to run HealthCheckProgram in seconds. 0 disables health checks.
##HealthCheckProgram=/hpc/slurm/healthcheck.sh # Pathname to health check script. Use script to mark node down or email admins.
InactiveLimit=0 # Interval in seconds after which a job allocation command like srun or salloc will terminate a job. Default is unlimited (zero).
KillWait=30 # Time in seconds between asking the job to shut down gracefully via SIGTERM and forcefully via SIGKILL. Default is 30 seconds.
##MessageTimeout=10 
ResvOverRun=0 # Amount of time in minutes to allow a job to finish executing if it's run out of time and still executing
MinJobAge=300 # The minimum age of a completed job in seconds before Slurm purges the job from its active database. The default is 300.
OverTimeLimit=5 # How many minutes a job can exceed its runtime before it is canceled. By default the job limit is a soft limit, this parameter sets a hard limit.
SlurmctldTimeout=120
SlurmdTimeout=300
##VSizeFactor=0 
Waittime=0
# 
# 
# SCHEDULING 
DefMemPerCPU=900 # Default memory allocated per CPU in MB
FastSchedule=1
MaxMemPerCPU=0 # Maximum memory allowed per CPU being used. Default is 0, unlimited. Requres accounting which samples memory use regularly.
SchedulerTimeSlice=30 # Number of seconds in each scheduler iteration for gang scheduling.
SchedulerType=sched/backfill # Scheduling type. backfill, builtin (FIFO) or hold. Hold will hold new jobs if /etc/slurm.hold exists, otherwise uses builtin.
SelectType=select/cons_res # Determine the resource seelction algorithm. cons_res allocates resources within a node individually.
##SelectTypeParameters=CR_Core_Memory # This tells the scheduler we would like to consume both cores and memory when scheduling.
SelectTypeParameters=CR_Core 
# 
# 
# JOB PRIORITY 
##PriorityFlags=FAIR_TREE # Flags to modify fairshare priority. FAIR_TREE makes children of accounts inherit fairshare priority of parent accounts.
##PriorityType=priority/multifactor # This enables fairshare
##PriorityDecayHalfLife=3-0 # How long a window the scheduler will consider someone's resource use in determining priority. Default value is 7-0, 7 days 0 hours.
##PriorityCalcPeriod=5 # This is the amount of time in minutes in which half-life decay is recalculated.
##PriorityFavorSmall=NO # This means the scheduler will prioritize large jobs over small jobs. 
##PriorityMaxAge=7-0 # This is the time at which the wait factor priority maxes out. Currently set to 7 days and 0 hours.
##PriorityUsageResetPeriod=NONE # This is used to set a hard limit on SUs consumed per year. Fairshare instead uses the PriorityDecayHalfLife setting. 
##PriorityWeightAge=1000 # Integer that determines the degree to which queue wait time influences priority
##PriorityWeightFairshare=100000 # Integer that determines the degree to which fair share influences priority
##PriorityWeightJobSize=100 # Integer that determines the degree to which job size determines priority
##PriorityWeightPartition=0 # Integer that determines the degree to which partition priority determines priority
##PriorityWeightQOS=0 # Integer that determines the degree to which QOS priority determines priority
# 
# 
# LOGGING AND ACCOUNTING 
AccountingStorageEnforce=associations,qos,limits,safe
AccountingStorageHost=qnode0
##AccountingStorageBackupHost=qslurmdb02
##AccountingStorageLoc=
##AccountingStoragePass=
##AccountingStoragePort=
##AccountingStorageTRES=gres/gpu
AccountingStorageType=accounting_storage/slurmdbd # Accounting method to use. filetxt uses the AccountingStorageLoc location. Slurmdbd uses a database specified in slurmdbd.conf.
AccountingStorageUser=slurm
AccountingStoreJobComment=YES
ClusterName=mini-Quest # The name by which this cluster is known in the Slurm accounting database.
##DebugFlags= 
##JobCompHost=
##JobCompLoc=
##JobCompPass=
##JobCompPort=
JobCompType=jobcomp/none
##JobCompUser=
##JobContainerType=job_container/none # Job container type to use for job tracking. Only used for Cray systems. 
JobAcctGatherFrequency=30
JobAcctGatherType=jobacct_gather/linux
JobAcctGatherParams=OverMemoryKill # TODO: Remove this once restored hard memory limits
MemLimitEnforce=YES # TODO: Remove this once restored hard memory limits
SlurmctldDebug=debug5 # Controls level of slurmctld debug. Maximum is debug5. Error only logs errors.
SlurmctldLogFile=/slurmctld_data/slurmctld.log # Where to save the slurmctld log file.
SlurmdDebug=debug5 # Controls level of slurmctld debug. Maximum is debug5. Error only logs errors.
SlurmdLogFile=/slurmd_data/slurmd.log # Where to save the slurmd log file.
#SlurmSchedLogFile= 
#SlurmSchedLogLevel= 
# 
# 
# POWER SAVE SUPPORT FOR IDLE NODES (optional) 
##SuspendProgram= # Node to hibernate or power off a node.
##ResumeProgram= # Program to bring a hibernating or powered off node back online.
##SuspendTimeout= # Maximum time for node to hibernate after SuspendProgram executes before the node is marked down.
##ResumeTimeout= # Maximum time for a node to resume (reboot or leave hibernation) before the scheduler marks the node down.
##ResumeRate= # Nodes per minute returned to service by ResumeProgram.
##SuspendExcNodes= # Nodes that should never be powered down. Use slurm.conf hostlist expression. Can use node[10-20]:4 to prevent four nodes within a range from powering down without specifying which nodes.
##SuspendExcParts= # Comma-delimeted list of partitions where nodes shouldn't be suspended.
##SuspendRate= # Nodes per minute that should be suspended using SuspendProgram.
##SuspendTime= # How many seconds a node should be idle before it is eligble to be suspended.
# 
# 
# COMPUTE NODES
NodeName=qnode1 CPUs=24 Sockets=2 CoresPerSocket=6 ThreadsPerCore=2 RealMemory=20000 State=UNKNOWN
NodeName=qnode2 CPUs=4 Sockets=1 CoresPerSocket=4 ThreadsPerCore=1 RealMemory=800 State=UNKNOWN
NodeName=qnode3 CPUs=4 Sockets=1 CoresPerSocket=4 ThreadsPerCore=1 RealMemory=800 State=UNKNOWN
PartitionName=normal Nodes=qnode1,qnode2,qnode3 Default=YES MaxTime=168:00:00 State=UP
#PartitionName=debug  Nodes=qnode2,qnode3 Default=YES MaxTime=08:00:00 State=UP
