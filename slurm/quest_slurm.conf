# slurm.conf file generated by configurator.html.
# Put this file on all nodes of your cluster.
# See the slurm.conf man page for more information.
#
ControlMachine=qslurm01 # This is the hostname of the scheduler 
ControlAddr=172.20.5.11 # This is the IP address of the scheduler (optional if you don't want it to use /etc/hosts)
BackupController=qslurm02 # This is the hostname of the backup scheduler
BackupAddr=172.20.5.12 # This is the IP address  of the backup scheduler (optional if you don't want it to use /etc/hosts)
# 
AuthType=auth/munge # How communications between nodes are secured. Can use none or munge, should always use munge.
CheckpointType=checkpoint/none # What integrated checkpoint tool to use. Could use ompi for OpenMPI version 1.3+
CryptoType=crypto/munge # Whether to use munge or ssl for cryptographic signature.
DisableRootJobs=NO # Whether to prevent root from running jobs 
EnforcePartLimits=ALL # Prevents jobs with invalid parameters from being submitted (rather than blocked)
Epilog=/hpc/slurm/snapshot.epilog # Script to run to clean up user processes at the end of sbatch job. Don't use the default one as it will kill all of a user's jobs on a node when one ends. Better to use cgroups and PAM for control.
#Prolog=/hpc/slurm/snapshot.prolog # Path of a program for slurmd to execute whenever asked to run a job step from a new job allocation.
#SrunEpilog= # Script to run to clean up user processes at the end of srun job. See note for Epilog.
SrunProlog=/hpc/slurm/srun.prolog # Prologue script to run for srun jobs.
#EpilogSlurmctld= # Epilogue script to run as slurm user on job completion/failure.
FirstJobId=1 # The first ID to assign to a job.
MaxJobId=999999 # The highest possible ID to assign to a job before it rolls over to assign the FirstJobId again.
GresTypes=gpu # Defines types of generic resources for nodes. Here we've defined gpus as a consumable resource. Also configured in gres.conf.
GroupUpdateForce=1 # 1 Checks which user group membership periodically for access to partitions. 0 only checks when /etc/group has been updated. This may be helpful if using the database to control access to queues.
#X11Parameters=local_xauthority # TODO: Restore this once restored native X11# Prevents X11 locks on Xauthority by using a temp file instead of /home/$USER/.Xauthority
GroupUpdateTime=600 # How often to check which users are members of groups allowed to use a partition. Time is in seconds.
JobCheckpointDir=/hpc/slurm/checkpoint # Location to store job checkpoint information. Should be writable by SlurmUser but not by regular users.
#JobCredentialPrivateKey= # Path to private key for authentication if not using munge.
#JobCredentialPublicCertificate= # Path to public key for authentication if not using munge.
#JobFileAppend=0 # Determines what to do if error/output files already exist for a job. 0 is truncate old info, 1 is append new info.
JobRequeue=1 # Controls whether batch jobs will be requeued. Can be overridden by user submit settings --requeue or --no-requeue.
JobSubmitPlugins=lua # Comma delimited list of job submission plugins. 
#KillOnBadExit=0 # Terminate job step immediately if any tasks have a non-0 exit code.
#LaunchType=launch/slurm # Special launch parameters for all-in-one clusters. Otherwise use slurm.
Licenses=mdcsw:600 # Specifies licenses that can be requested by a job. Jobs can still use licenses that aren't explicitly requested.
MailProg=/usr/bin/mail # Path to mail program used to send notification emails.
MaxJobCount=300000 # Maximum number of jobs before the scheduler begins rejecting submissions. Per documentation, performance can suffer if more than a few hundred thousand jobs are being run. 
MaxArraySize=5000 # Maximum array size
MaxStepCount=40000 # Maximum number of steps any job can initiate. This is intended to prevent bad scripts from overwhelming nodes and the scheduler. Default is 40,000.
MaxTasksPerNode=512 # Maximum number of tasks Slurm will allow a job step to spawn on a single node. Default is 512, maximum is 65533.
MpiDefault=none # Default MPI type to use, can be overridden by job. Options are openmpi, pmi2, pmix, and none (which is the default, and works for many other versions of MPI).
#MpiParams=ports=#-# # Port range used by older versions of OpenMPI. If using a modern version of OpenMPI, leave this blank for better performance. 
#PluginDir= # Colon-separated list of directories where to look for Slurm plugins. Default is /usr/local/lib/slurm.
#PlugStackConfig= # Location of Slurm stackable Plugin Architecture for Node job (K)control (SPANK). Default location is plugstack.conf in same directory as slurm.conf.
#PrivateData= # Comma delimeted list of information to hide from users. By default users can see everything. Root and SlurmUser can always see everything.
ProctrackType=proctrack/cgroup # Similar to TaskPlugin, how slurm manages processes on a node. Default is using cgroups.
PrologFlags=contain,alloc#,X11 # TODO: Restore X11 once using native capability # Needed for PAM to prevent users from sshing without a job
#PrologSlurmctld= # Path for a program for slurmctld to execute before granting a new job allocation.
#PropagatePrioProcess=0 # Determines the scheduling priority (nice value) of user spawned tasks. 0 inherit from slurmd, 1 inherit from srun or sbatch but no higher than slurmd, and 2 is the same as 1 but one priority higher than slurmd. Default is 0.
PropagateResourceLimits=NONE # Which limits (from ulimit) to inherit from the login node and apply to the job. Default value is ALL.
#PropagateResourceLimitsExcept= # Which limits not to inherit from the login node and apply to the job.
#RebootProgram= # Program to be executed on each compute node to reboot it. Invoked on idle nodes after scontrol reboot_nodes or a job is submitted with --reboot option. Will wait for node to become idle before executing.
ReturnToService=2 # Controls when a down node is returned to service. 0: manually, 1: recovery after being non-responsive, 2: recovery after any circumstance, including bad memory, etc.
#SallocDefaultCommand= # Default command to execute when running salloc.
SlurmctldPidFile=/var/run/slurmctld.pid # Must be writable by root, preferable also writable by SlurmUser. File must be available for both primary and backup schedulers.
SlurmctldPort=6817 # The port slurmctld communicates over. 6817 is the default.
SlurmdPidFile=/var/run/slurmd.pid # Location of slurmd pid file for nodes.
SlurmdPort=6818 # The port slurmd communicates over. 6818 is the default.
SlurmdSpoolDir=/var/spool/slurmd # Path where slurmd daemon state and batch job script information are written.
SlurmUser=slurm # User that slurmctld executes as. Recommended to use a service account rather than root.
SlurmdUser=root # User that the slurmd daemon executes as for communication via munge as well as executing UnkillableStepProgram. Usually root.
StateSaveLocation=/hpc/slurm/spool # Path where the scheduler saves its state. Should be readable and writable by both the primary and backup scheduler. SlurmUser should have access.
SwitchType=switch/none # This is used for special types of switches, like Cray and IBM all-in-one clusters. For Ethernet or Infiniband, use none.
#TaskEpilog= # Path to program to execute at completion of each job task.
TaskPlugin=task/cgroup # How slurm manages resources on a node. cgroup will use cgroups to constrain jobs to the resources they've requested.
#TaskPluginParam= # Comma separated list of optional parameters for the TaskPlugin.
#TaskProlog= # Path to program to run at the beginning of each job task.
TopologyPlugin=topology/none # Identifies plugin for determinine network topology and minimizing network contention. If tree, use topology.conf.
TmpFS=/tmp # Where jobs should point to for temporary storage. Default value is /tmp.
#TrackWCKey=no 
#TreeWidth= 
#UnkillableStepProgram=  # Program to execute if a task isn't killed after sending it a SIGKILL and waiting UnlikkableStepTimeout.
UnkillableStepTimeout=60 # Amount of time in seconds to wait for a task to be killed after sending it a SIGKILL command before calling UnkillableStepProgram and marking the node down.
#UsePAM=0 
PreemptMode=REQUEUE # What to do with pre-empted jobs. Default is OFF.
PreemptType=preempt/partition_prio # How pre-emption should function. Can either use qos or priorityTier based pre-emption.
# 
# 
# TIMERS 
#BatchStartTimeout=10 
#CompleteWait=0 
#EpilogMsgTime=2000 
#GetEnvTimeout=2 
HealthCheckInterval=1800 # How often to run HealthCheckProgram in seconds. 0 disables health checks.
HealthCheckProgram=/hpc/slurm/healthcheck.sh # Pathname to health check script. Use script to mark node down or email admins.
InactiveLimit=0 # Interval in seconds after which a job allocation command like srun or salloc will terminate a job. Default is unlimited (zero).
KillWait=30 # Time in seconds between asking the job to shut down gracefully via SIGTERM and forcefully via SIGKILL. Default is 30 seconds.
#MessageTimeout=10 
ResvOverRun=0 # Amount of time in minutes to allow a job to finish executing if it's run out of time and still executing
MinJobAge=300 # The minimum age of a completed job in seconds before Slurm purges the job from its active database. The default is 300.
OverTimeLimit=5 # How many minutes a job can exceed its runtime before it is canceled. By default the job limit is a soft limit, this parameter sets a hard limit.
SlurmctldTimeout=120
SlurmdTimeout=300
#VSizeFactor=0 
Waittime=0
# 
# 
# SCHEDULING 
DefMemPerCPU=3266 # Default memory allocated per CPU in MB
FastSchedule=1
MaxMemPerCPU=0 # Maximum memory allowed per CPU being used. Default is 0, unlimited. Requres accounting which samples memory use regularly.
SchedulerTimeSlice=30 # Number of seconds in each scheduler iteration for gang scheduling.
SchedulerType=sched/backfill # Scheduling type. backfill, builtin (FIFO) or hold. Hold will hold new jobs if /etc/slurm.hold exists, otherwise uses builtin.
SelectType=select/cons_res # Determine the resource seelction algorithm. cons_res allocates resources within a node individually.
SelectTypeParameters=CR_Core_Memory # This tells the scheduler we would like to consume both cores and memory when scheduling.
# 
# 
# JOB PRIORITY 
PriorityFlags=FAIR_TREE # Flags to modify fairshare priority. FAIR_TREE makes children of accounts inherit fairshare priority of parent accounts.
PriorityType=priority/multifactor # This enables fairshare
PriorityDecayHalfLife=3-0 # How long a window the scheduler will consider someone's resource use in determining priority. Default value is 7-0, 7 days 0 hours.
PriorityCalcPeriod=5 # This is the amount of time in minutes in which half-life decay is recalculated.
PriorityFavorSmall=NO # This means the scheduler will prioritize large jobs over small jobs. 
PriorityMaxAge=7-0 # This is the time at which the wait factor priority maxes out. Currently set to 7 days and 0 hours.
PriorityUsageResetPeriod=NONE # This is used to set a hard limit on SUs consumed per year. Fairshare instead uses the PriorityDecayHalfLife setting. 
PriorityWeightAge=1000 # Integer that determines the degree to which queue wait time influences priority
PriorityWeightFairshare=100000 # Integer that determines the degree to which fair share influences priority
PriorityWeightJobSize=100 # Integer that determines the degree to which job size determines priority
PriorityWeightPartition=0 # Integer that determines the degree to which partition priority determines priority
PriorityWeightQOS=0 # Integer that determines the degree to which QOS priority determines priority
# 
# 
# LOGGING AND ACCOUNTING 
AccountingStorageEnforce=associations,qos,limits,safe
AccountingStorageHost=qslurmdb01
AccountingStorageBackupHost=qslurmdb02
#AccountingStorageLoc=
#AccountingStoragePass=
#AccountingStoragePort=
AccountingStorageTRES=gres/gpu
AccountingStorageType=accounting_storage/slurmdbd # Accounting method to use. filetxt uses the AccountingStorageLoc location. Slurmdbd uses a database specified in slurmdbd.conf.
AccountingStorageUser=slurm
AccountingStoreJobComment=YES
ClusterName=quest # The name by which this cluster is known in the Slurm accounting database.
#DebugFlags= 
#JobCompHost=
#JobCompLoc=
#JobCompPass=
#JobCompPort=
JobCompType=jobcomp/none
#JobCompUser=
#JobContainerType=job_container/none # Job container type to use for job tracking. Only used for Cray systems. 
JobAcctGatherFrequency=30
JobAcctGatherType=jobacct_gather/linux
JobAcctGatherParams=OverMemoryKill # TODO: Remove this once restored hard memory limits
MemLimitEnforce=YES # TODO: Remove this once restored hard memory limits
SlurmctldDebug=error # Controls level of slurmctld debug. Maximum is debug5. Error only logs errors.
SlurmctldLogFile=/var/log/slurm/slurmctld.log # Where to save the slurmctld log file.
SlurmdDebug=error # Controls level of slurmctld debug. Maximum is debug5. Error only logs errors.
SlurmdLogFile=/var/log/slurm/slurmd.log # Where to save the slurmd log file.
#SlurmSchedLogFile= 
#SlurmSchedLogLevel= 
# 
# 
# POWER SAVE SUPPORT FOR IDLE NODES (optional) 
#SuspendProgram= # Node to hibernate or power off a node.
#ResumeProgram= # Program to bring a hibernating or powered off node back online.
#SuspendTimeout= # Maximum time for node to hibernate after SuspendProgram executes before the node is marked down.
#ResumeTimeout= # Maximum time for a node to resume (reboot or leave hibernation) before the scheduler marks the node down.
#ResumeRate= # Nodes per minute returned to service by ResumeProgram.
#SuspendExcNodes= # Nodes that should never be powered down. Use slurm.conf hostlist expression. Can use node[10-20]:4 to prevent four nodes within a range from powering down without specifying which nodes.
#SuspendExcParts= # Comma-delimeted list of partitions where nodes shouldn't be suspended.
#SuspendRate= # Nodes per minute that should be suspended using SuspendProgram.
#SuspendTime= # How many seconds a node should be idle before it is eligble to be suspended.
# 
# 
# COMPUTE NODES
#
NodeName=qnode[4002-4005,4007,4010-4011,4014,4016-4022,4024-4026,4028-4031,4033-4055,4057-4059,4061-4078,4083,4085-4094,4096-4111,4127-4151,4153-4159,4161-4184,4186-4194,4196-4223,4230-4236] CPUs=20 Boards=1 SocketsPerBoard=2 CoresPerSocket=10 ThreadsPerCore=1 RealMemory=128745 MemSpecLimit=4096 Feature=quest4 Weight=40 State=UNKNOWN
#
NodeName=qnode[5002-5024,5028-5072,5074-5108,5110-5218] CPUs=24 Boards=1 SocketsPerBoard=2 CoresPerSocket=12 ThreadsPerCore=1 RealMemory=128054 MemSpecLimit=4096 Feature=quest5 Weight=50 State=UNKNOWN
#
NodeName=qnode[5219-5233,5235-5238,6001-6144,6701-6877] CPUs=28 Boards=1 SocketsPerBoard=2 CoresPerSocket=14 ThreadsPerCore=1 RealMemory=128341 MemSpecLimit=4096 Feature=quest6 Weight=60 State=UNKNOWN
#
NodeName=qhimem[0002-0006] CPUs=24 Boards=1 SocketsPerBoard=2 CoresPerSocket=12 ThreadsPerCore=1 RealMemory=773600 MemSpecLimit=4096 Feature=quest5 Weight=1000 State=UNKNOWN
#
NodeName=qhimem[0007-0011] CPUs=28 Boards=1 SocketsPerBoard=2 CoresPerSocket=14 ThreadsPerCore=1 RealMemory=515389 MemSpecLimit=4096 Feature=quest6 Weight=1000 State=UNKNOWN
#
NodeName=qhimem[0012-0013] CPUs=28 Boards=1 SocketsPerBoard=2 CoresPerSocket=14 ThreadsPerCore=1 RealMemory=514727 MemSpecLimit=4096 Feature=quest8 Weight=1000 State=UNKNOWN
#
NodeName=qnode[8001-8072] CPUs=28 Boards=1 SocketsPerBoard=2 CoresPerSocket=14 ThreadsPerCore=1 RealMemory=95545 MemSpecLimit=4096 Feature=quest8 Weight=80 State=UNKNOWN
#
NodeName=qnode[8101-8146] CPUs=28 Boards=1 SocketsPerBoard=2 CoresPerSocket=14 ThreadsPerCore=1 RealMemory=192309 Feature=quest8 Weight=80 State=UNKNOWN
#
Nodename=qgpu[5001-5004] CPUs=24 Boards=1 SocketsPerBoard=2 CoresPerSocket=12 ThreadsPerCore=1 RealMemory=128654 Gres=gpu:k40:2 MemSpecLimit=4096 Feature=quest5 Weight=1000 State=UNKNOWN
#
Nodename=qgpu[5005-5009] CPUs=24 Boards=1 SocketsPerBoard=2 CoresPerSocket=12 ThreadsPerCore=1 RealMemory=128649 Gres=gpu:k10:2 MemSpecLimit=4096 Feature=quest5 Weight=1000 State=UNKNOWN
#
Nodename=qgpu[5010-5015] CPUs=24 Boards=1 SocketsPerBoard=2 CoresPerSocket=12 ThreadsPerCore=1 RealMemory=128649 Gres=gpu:k80:2 MemSpecLimit=4096 Feature=quest5 Weight=1000 State=UNKNOWN
#
NodeName=qgpu[6001-6015] CPUs=28 Boards=1 SocketsPerBoard=2 CoresPerSocket=14 ThreadsPerCore=1 RealMemory=128335 Gres=gpu:k80:2 MemSpecLimit=4096 Feature=quest6 Weight=1000 State=UNKNOWN
#
NodeName=qgpu[6016-6038] CPUs=28 Boards=1 SocketsPerBoard=2 CoresPerSocket=14 ThreadsPerCore=1 RealMemory=128335 Gres=gpu:p100:2 MemSpecLimit=4096 Feature=quest6 Weight=1000 State=UNKNOWN
#
NodeName=qgpu6039 CPUs=28 Boards=1 SocketsPerBoard=2 CoresPerSocket=14 ThreadsPerCore=1 RealMemory=127479 Gres=gpu:p100:2 MemSpecLimit=4096 Feature=quest6 Weight=1000 State=UNKNOWN
#
NodeName=qgpu[8101-8102] CPUs=28 Boards=1 SocketsPerBoard=2 CoresPerSocket=14 ThreadsPerCore=1 RealMemory=192309 Gres=gpu:v100:2 MemSpecLimit=4096 Feature=quest8 Weight=1000 State=UNKNOWN
#
# PARTITIONS
PartitionName=DEFAULT TRESBillingWeights="CPU=4.0,Mem=3.2G,gres/gpu=12.0" DenyQOS=expired State=UP
#
PartitionName=short Nodes=qnode[5002-5024,5028-5072,5074-5108,5110-5218,5219-5222,5229-5233,5235-5238,6001-6004,6006-6016,6018-6041,6047,6052-6084,6086-6089,6092-6099,6101-6102,6141,8001-8072,6701-6877] DEFAULT=YES State=UP MaxTime=04:00:00 PreemptMode=OFF
#PartitionName=short Nodes=qnode[5002-5024,5028-5072,5074-5108,5110-5218,5219-5222,5229-5233,5235-5238,6001-6004,6006-6016,6018-6041,6047,6052-6084,6086-6089,6092-6099,6101-6102,6141,8001-8072,6701-6877] DEFAULT=YES State=UP MaxTime=04:00:00 AllowQos=normal PreemptMode=OFF
#
PartitionName=normal Nodes=qnode[5002-5024,5028-5072,5074-5108,5110-5218,5219-5222,5229-5233,5235-5238,6001-6004,6006-6016,6018-6041,6047,6052-6084,6086-6089,6092-6099,6101-6102,6141,8001-8072,6701-6877] State=UP MaxTime=2-00:00:00 PreemptMode=OFF
#PartitionName=normal Nodes=qnode[5003,5009-5010,5015-5018,5020-5024,5028-5031,5034-5035,5037-5044,5046-5072,5074,5088,5090,5092,5094-5100,5103-5108,5111-5114,5117,5136,5148,5166,5170,5180,5184,5199,5204,5229,5231-5233,5235,6020-6021,6026-6027,6030,6041,6047,6057,6084,6718-6724,6768-6772,6815,6822,6824,6826-6832,6141,8023,8044-8072] State=UP MaxTime=2-00:00:00 AllowQos=normal PreemptMode=OFF
#
PartitionName=long Nodes=qnode[5002-5024,5028-5072,5074-5108,5110-5218,5219-5222,5229-5233,5235-5238,6001-6004,6006-6016,6018-6041,6047,6052-6084,6086-6089,6092-6099,6101-6102,6141,8001-8072,6701-6877] State=UP MaxTime=2-00:00:00 PreemptMode=OFF
#PartitionName=long Nodes=qnode[5003,5009-5010,5015-5018,5020-5024,5028-5031,5034-5035,5037-5044,5046-5072,5074,5088,5090,5092,5094-5100,5103-5108,5111-5114,5117,5136,5148,5166,5170,5180,5184,5199,5204,5229,5231-5233,5235,6020-6021,6026-6027,6030,6041,6047,6057,6084,6718-6724,6768-6772,6815,6822,6824,6826-6832,6141,8023,8044-8072] State=UP MaxTime=2-00:00:00 AllowQos=normal PreemptMode=OFF
#
PartitionName=gengpu Nodes=qgpu6039 State=UP AllowGroups=gengpu AllowAccounts=gengpu PreemptMode=OFF
#PartitionName=gengpu Nodes=qgpu[5001-5004] State=UP AllowGroups=gengpu AllowAccounts=gengpu PreemptMode=OFF
#
PartitionName=a9009 Nodes=ALL State=UP AllowGroups=a9009 AllowAccounts=a9009 PriorityTier=2 PreemptMode=OFF
#
PartitionName=b1003 Nodes=ALL State=UP MaxTime=2-00:00:00 AllowGroups=b1003 AllowAccounts=b1003 PriorityTier=2 PreemptMode=OFF
#PartitionName=b1003 Nodes=qnode[5219-5222,5230,5236-5238,6001-6002,6010-6013,6016,6025,6031-6038,6040,6054-6055,6706-6709] State=UP AllowGroups=b1003 AllowAccounts=b1003 PriorityTier=2 PreemptMode=OFF
#
PartitionName=b1004 Nodes=ALL State=UP MaxTime=2-00:00:00 AllowGroups=b1004 AllowAccounts=b1004 PriorityTier=2 PreemptMode=OFF
#PartitionName=b1004 Nodes=ALL State=qnode[6710-6711,6062,6064-6080] AllowGroups=b1004 AllowAccounts=b1004 PriorityTier=2 PreemptMode=OFF
#
PartitionName=b1005 Nodes=ALL State=UP MaxTime=2-00:00:00 AllowGroups=b1005 AllowAccounts=b1005 PriorityTier=2 PreemptMode=OFF
#PartitionName=b1005 Nodes=qnode[6082,6086-6089,6093,6039,6095-6099,6014-6015,6101,6024,6028-6029,6052-6053] State=UP AllowGroups=b1005 AllowAccounts=b1005 PriorityTier=2 PreemptMode=OFF
#
PartitionName=b1007 Nodes=ALL State=UP MaxTime=2-00:00:00 AllowGroups=b1007 AllowAccounts=b1007 PriorityTier=2 PreemptMode=OFF
#PartitionName=b1007 Nodes=qnode[5040-5042] State=UP AllowGroups=b1007 AllowAccounts=b1007 PriorityTier=2 PreemptMode=OFF
#
PartitionName=b1010 Nodes=ALL State=UP MaxTime=2-00:00:00 AllowGroups=b1010 AllowAccounts=b1010 PriorityTier=2 PreemptMode=OFF
#PartitionName=b1010 Nodes=qnode5020 State=UP AllowGroups=b1010 AllowAccounts=b1010 PriorityTier=2 PreemptMode=OFF
#
PartitionName=conference Nodes=ALL State=UP MaxTime=2-00:00:00 AllowGroups=sbc538,cpp958,vka354,clr345,mjz672,cbk632,mrj2142,krs1878,ldz0482 AllowAccounts=b1011 PriorityTier=2 PreemptMode=OFF
#PartitionName=conference Nodes=qnode[6005,6103-6140,6142-6144,5223-5228,6085,6090-6091,6100] State=UP AllowGroups=sbc538,cpp958,vka354,clr345,mjz672,cbk632,mrj2142,krs1878,ldz0482 AllowAccounts=b1011 PriorityTier=2 PriorityTier=2 Priority=200000 PreemptMode=OFF
PartitionName=ligo Nodes=ALL State=UP MaxTime=2-00:00:00 AllowGroups=vka354,cpp958,clr345,sbc538,sch656,mjz672,kjc004,jll839,cpp958,klk967,ncw582,syr904,kmb799,eac3720,cbk632,far946,krs1878,mrj2142,ldz0482 AllowAccounts=b1011 PriorityTier=2 PreemptMode=OFF
#PartitionName=ligo Nodes=qnode[6005,6085,6090-6091,6100,6103-6140,6142-6144] State=UP AllowGroups=vka354,cpp958,clr345,sbc538,sch656,mjz672,kjc004,jll839,cpp958,klk967,ncw582,syr904,kmb799,eac3720,cbk632,far946,krs1878,mrj2142,ldz0482 AllowAccounts=b1011 PriorityTier=2 PriorityTier=2 Priority=100000 MaxTime=2-00:00:00 PreemptMode=OFF
PartitionName=astro Nodes=ALL State=UP MaxTime=2-00:00:00 AllowGroups=mlk667,paw663,cag352,jll839,mga182,gnovak,jfj4455,amg416,jfy279,cbk632,smj161,hjp637,pic6628,zhh137,ahx065,nsr857,aam3503,mjb199,cvd161,ijr0145,xhr5580,egc2975,kpm1266 AllowAccounts=b1011 PriorityTier=2 PreemptMode=OFF
#PartitionName=astro Nodes=qnode[6005,6103-6140,6142-6144,5223-5228,6085,6090-6091,6100] State=UP MaxTime=7-00:00:00 AllowGroups=mlk667,paw663,cag352,jll839,mga182,gnovak,jfj4455,amg416,jfy279,cbk632,smj161,hjp637,pic6628,zhh137,ahx065,nsr857,aam3503,mjb199,cvd161,ijr0145,xhr5580,egc2975,kpm1266 AllowAccounts=b1011 PriorityTier=2 PriorityTier=2 Priority=50000 PreemptMode=OFF
PartitionName=collab Nodes=ALL State=UP MaxTime=2-00:00:00 AllowGroups=elu294,mhn353,hwf586,zwn162,hmf576,nre739,sbz550,hsa7815,zwt002 AllowAccounts=b1011 PriorityTier=2 PreemptMode=OFF
#PartitionName=collab Nodes=qnode[6005,6103-6140,6142-6144,5223-5228,6085,6090-6091,6100] State=UP MaxTime=7-00:00:00 AllowGroups=elu294,mhn353,hwf586,zwn162,hmf576,nre739,sbz550,hsa7815,zwt002 AllowAccounts=b1011 PriorityTier=2 PriorityTier=2 Priority=5000 PreemptMode=OFF
PartitionName=grailgpu3 Nodes=ALL State=UP MaxTime=2-00:00:00 AllowGroups=vka354,clr345,sbc538,jfy279,amd616 AllowAccounts=b1011 PriorityTier=2 PreemptMode=OFF
#PartitionName=grailgpu3 Nodes=qgpu[6006-6014] State=UP AllowGroups=vka354,clr345,sbc538,jfy279,amd616 AllowAccounts=b1011 PriorityTier=2 PriorityTier=2 Priority=150000 PreemptMode=OFF
PartitionName=grailgpu4 Nodes=ALL State=UP MaxTime=2-00:00:00 AllowGroups=sbz550,sbc538,nre739,mlk667,mjw2704 AllowAccounts=b1011 PriorityTier=2 PreemptMode=OFF
#PartitionName=grailgpu4 Nodes=qgpu6015 State=UP AllowGroups=sbz550,sbc538,nre739,mlk667,mjw2704 AllowAccounts=b1011 PriorityTier=2 PriorityTier=2 Priority=150000 PreemptMode=OFF
PartitionName=grailhimem Nodes=ALL State=UP MaxTime=2-00:00:00 AllowGroups=vka354,clr345,far946,elu294,mhn353,hwf586,cag352,sbc538,gnovak,mga182,zwn162,amg416,jfy279,sch656,mjz672,nsr857,smj161,kjc004,cbk632,cpp958,jll839,zhh137,klk967,ahx065,hmf576,ncw582,nre739,sbz550,ejm553,jfj4455,paw663,mlk667,hsa7815,hjp637,syr904,zwt002,eac3720,aam3503 AllowAccounts=b1011 PriorityTier=2 PreemptMode=OFF
#PartitionName=grailhimem Nodes=qhimem[0007-0009] State=UP AllowGroups=vka354,clr345,far946,elu294,mhn353,hwf586,cag352,sbc538,gnovak,mga182,zwn162,amg416,jfy279,sch656,mjz672,nsr857,smj161,kjc004,cbk632,cpp958,jll839,zhh137,klk967,ahx065,hmf576,ncw582,nre739,sbz550,ejm553,jfj4455,paw663,mlk667,hsa7815,hjp637,syr904,zwt002,eac3720,aam3503 AllowAccounts=b1011 PriorityTier=2 PriorityTier=2 Priority=5000 PreemptMode=OFF
#
PartitionName=b1012 Nodes=ALL State=UP MaxTime=2-00:00:00 AllowGroups=b1012 AllowAccounts=b1012 PriorityTier=2 PreemptMode=OFF
#PartitionName=b1012 Nodes=qnode5012 State=UP AllowGroups=b1012 AllowAccounts=b1012 PriorityTier=2 PreemptMode=OFF
#
PartitionName=b1013 Nodes=ALL State=UP MaxTime=2-00:00:00 AllowGroups=b1013 AllowAccounts=b1013 PriorityTier=2 PreemptMode=OFF
#PartitionName=b1013 Nodes=qnode[6874-6875] State=UP AllowGroups=b1013 AllowAccounts=b1013 PriorityTier=2 PreemptMode=OFF
#
PartitionName=b1015 Nodes=ALL State=UP MaxTime=2-00:00:00 AllowGroups=b1015 AllowAccounts=b1015 PriorityTier=2 PreemptMode=OFF
#PartitionName=b1015 Nodes=qnode6102 State=UP AllowGroups=b1015 AllowAccounts=b1015 PriorityTier=2 PreemptMode=OFF
#
PartitionName=b1016 Nodes=ALL State=UP MaxTime=2-00:00:00 AllowGroups=b1016 AllowAccounts=b1016 PriorityTier=2 PreemptMode=OFF
#PartitionName=b1016 Nodes=ALL State=qnode[8006-8010] AllowGroups=b1016 AllowAccounts=b1016 PriorityTier=2 PreemptMode=OFF
#
PartitionName=b1017 Nodes=ALL State=UP MaxTime=2-00:00:00 AllowGroups=b1017 AllowAccounts=b1017 PriorityTier=2 PreemptMode=OFF
#PartitionName=b1017 Nodes=ALL State=qnode[6731-6738] AllowGroups=b1017 AllowAccounts=b1017 PriorityTier=2 PreemptMode=OFF
#
PartitionName=b1020 Nodes=ALL State=UP MaxTime=2-00:00:00 AllowGroups=b1020 AllowAccounts=b1020 PriorityTier=2 PreemptMode=OFF
#PartitionName=b1020 Nodes=qnode[6725-6728] State=UP AllowGroups=b1020 AllowAccounts=b1020 PriorityTier=2 PreemptMode=OFF
#
PartitionName=b1021 Nodes=ALL State=UP MaxTime=2-00:00:00 AllowGroups=b1021 AllowAccounts=b1021 PriorityTier=2 PreemptMode=OFF
#PartitionName=b1021 Nodes=qnode[6816-6821,6823,6830-6832,6853-6871,6876-6877,6836-6837,6825] State=UP AllowGroups=b1021 AllowAccounts=b1021 PriorityTier=2 PreemptMode=OFF
#
PartitionName=b1022 Nodes=ALL State=UP MaxTime=2-00:00:00 AllowGroups=b1022 AllowAccounts=b1022 PriorityTier=2 PreemptMode=OFF
#PartitionName=b1022 Nodes=qnode[6739-6745] State=UP AllowGroups=b1022 AllowAccounts=b1022 PriorityTier=2 PreemptMode=OFF
#
PartitionName=b1023 Nodes=ALL State=UP MaxTime=2-00:00:00 AllowGroups=b1023 AllowAccounts=b1023 PriorityTier=2 PreemptMode=OFF
#PartitionName=b1023 Nodes=qnode[5075-5087,5089,5128] State=UP AllowGroups=b1023 AllowAccounts=b1023 PriorityTier=2 PreemptMode=OFF
#
PartitionName=b1024 Nodes=ALL State=UP MaxTime=2-00:00:00 AllowGroups=b1024 AllowAccounts=b1024 PriorityTier=2 PreemptMode=OFF
#PartitionName=b1024 Nodes=qnode[6729-6730] State=UP AllowGroups=b1024 AllowAccounts=b1024 PriorityTier=2 PreemptMode=OFF
#
PartitionName=b1025 Nodes=ALL State=UP MaxTime=2-00:00:00 AllowGroups=b1025 AllowAccounts=b1025 PriorityTier=2 PreemptMode=OFF
#PartitionName=b1025 Nodes=qnode[6746-6748] State=UP AllowGroups=b1025 AllowAccounts=b1025 PriorityTier=2 PreemptMode=OFF
#
PartitionName=b1026 Nodes=ALL State=UP MaxTime=2-00:00:00 AllowGroups=b1026 AllowAccounts=b1026 PriorityTier=2 PreemptMode=OFF
#PartitionName=b1026 Nodes=qhimem[0002-0005,qnode[6773-6814,6701-6705,6838,6839-6845,6833-6835] State=UP AllowGroups=b1026 AllowAccounts=b1026 PriorityTier=2 PreemptMode=OFF
#
PartitionName=b1027 Nodes=ALL State=UP MaxTime=2-00:00:00 AllowGroups=b1027 AllowAccounts=b1027 PriorityTier=2 PreemptMode=OFF
#PartitionName=b1027 Nodes=qnode[8001-8005,8011-8022,8024-8043] State=UP AllowGroups=b1027 AllowAccounts=b1027 PriorityTier=2 PreemptMode=OFF
#
PartitionName=b1028 Nodes=ALL State=UP MaxTime=2-00:00:00 AllowGroups=b1028 AllowAccounts=b1028 PriorityTier=2 PreemptMode=OFF
#PartitionName=b1028 Nodes=qnode[5120,5011] State=UP AllowGroups=b1028 AllowAccounts=b1028 PriorityTier=2 PreemptMode=OFF
#
PartitionName=b1030 Nodes=ALL State=UP MaxTime=2-00:00:00 AllowGroups=b1030 AllowAccounts=b1030 PriorityTier=2 PreemptMode=OFF
#PartitionName=b1030 Nodes=qgpu[5005-5015,6001-6005,6016-6038] State=UP AllowGroups=b1030 AllowAccounts=b1030 PriorityTier=2 PreemptMode=OFF
#
PartitionName=b1033 Nodes=ALL State=UP MaxTime=2-00:00:00 AllowGroups=b1033 AllowAccounts=b1033 PriorityTier=2 PreemptMode=OFF
#PartitionName=b1033 Nodes=qnode[6083,6081] State=UP AllowGroups=b1033 AllowAccounts=b1033 PriorityTier=2 PreemptMode=OFF
#
PartitionName=b1036 Nodes=ALL State=UP MaxTime=2-00:00:00 AllowGroups=b1036 AllowAccounts=b1036 PriorityTier=2 PreemptMode=OFF
#PartitionName=b1036 Nodes=qnode[5002,5006-5007,5012-5014,5091] State=UP AllowGroups=b1036 AllowAccounts=b1036 PriorityTier=2 PreemptMode=OFF
#
PartitionName=b1038 Nodes=ALL State=UP MaxTime=2-00:00:00 AllowGroups=b1038 AllowAccounts=b1038 PriorityTier=2 PreemptMode=OFF
#PartitionName=b1038 Nodes=qnode5115 State=UP AllowGroups=b1038 AllowAccounts=b1038 PriorityTier=2 PreemptMode=OFF
#
PartitionName=b1039 Nodes=ALL State=UP MaxTime=2-00:00:00 AllowGroups=b1039 AllowAccounts=b1039 PriorityTier=2 PreemptMode=OFF
#PartitionName=b1039 Nodes=qnode5093 State=UP AllowGroups=b1039 AllowAccounts=b1039 PriorityTier=2 PreemptMode=OFF
#
PartitionName=b1041 Nodes=ALL State=UP MaxTime=2-00:00:00 AllowGroups=b1041 AllowAccounts=b1041 PriorityTier=2 PreemptMode=OFF
#PartitionName=b1041 Nodes=qhimem0006 State=UP AllowGroups=b1041 AllowAccounts=b1041 PriorityTier=2 PreemptMode=OFF
#
PartitionName=genomics Nodes=ALL State=UP MaxTime=48:00:00 MaxNodes=10 AllowGroups=b1042 AllowAccounts=b1042 PriorityTier=2 PreemptMode=OFF
#PartitionName=genomics Nodes=qnode[5032,5045,5101,5008,5118-5119,5121-5127,5129,5130-5135,5137-5147,5149-5165,5167-5169,5171-5179,5102,5181-5183,5185-5198,5200-5203,5033,5205-5218,5110,5116,5036] State=UP MaxTime=48:00:00 MaxNodes=10 AllowGroups=b1042 AllowAccounts=b1042 PriorityTier=2 PreemptMode=OFF
PartitionName=omics Nodes=ALL State=UP MaxTime=48:00:00 AllowGroups=b1042 AllowAccounts=b1042 PriorityTier=2 PreemptMode=OFF
#PartitionName=omics Nodes=qnode[5032,5045,5101,5008,5118-5119,5121-5127,5129,5130-5135,5137-5147,5149-5165,5167-5169,5171-5179,5102,5181-5183,5185-5198,5200-5203,5033,5205-5218,5110,5116,5036] State=UP MaxTime=336:00:00 AllowGroups=b1042 AllowAccounts=b1042 PriorityTier=2 PreemptMode=OFF
PartitionName=genomicsburst Nodes=ALL State=UP MaxTime=48:00:00 MaxNodes=70 AllowGroups=b1042 AllowAccounts=b1042 PriorityTier=2 PreemptMode=OFF
#PartitionName=genomicsburst Nodes=qnode[5032,5045,5101,5008,5118-5119,5121-5127,5129,5130-5135,5137-5147,5149-5165,5167-5169,5171-5179,5102,5181-5183,5185-5198,5200-5203,5033,5205-5218,5110,5116,5036] State=UP MaxTime=336:00:00 MaxNodes=70 AllowGroups=b1042 AllowAccounts=b1042 PriorityTier=2 PreemptMode=OFF
PartitionName=genomicsguest Nodes=ALL State=UP MaxTime=48:00:00 MaxNodes=10 AllowGroups=b1042 AllowAccounts=b1042 PriorityTier=2 PreemptMode=OFF
#PartitionName=genomicsguest Nodes=qnode[5032,5045,5101,5008,5118-5119,5121-5127,5129,5130-5135,5137-5147,5149-5165,5167-5169,5171-5179,5102,5181-5183,5185-5198,5200-5203,5033,5205-5218,5110,5116,5036] State=UP MaxTime=48:00:00 MaxNodes=10 AllowGroups=b1042 AllowAccounts=b1042 PriorityTier=2 PreemptMode=OFF
PartitionName=genomicslong Nodes=ALL State=UP MaxTime=48:00:00 AllowGroups=b1042 AllowAccounts=b1042 PriorityTier=2 PreemptMode=OFF
#PartitionName=genomicslong Nodes=qnode[5032,5045,5101,5008,5118-5119,5121-5127,5129,5130-5135,5137-5147,5149-5165,5167-5169,5171-5179,5102,5181-5183,5185-5198,5200-5203,5033,5205-5218,5110,5116,5036] State=UP MaxTime=240:00:00 AllowGroups=b1042 AllowAccounts=b1042 PriorityTier=2 PreemptMode=OFF
PartitionName=genomicsguestex Nodes=ALL State=UP MaxTime=48:00:00 AllowGroups=b1042 AllowAccounts=b1042 PriorityTier=2 PreemptMode=OFF
#PartitionName=genomicsguestex Nodes=qnode[5032,5045,5101,5008,5118-5119,5121-5127,5129,5130-5135,5137-5147,5149-5165,5167-5169,5171-5179,5102,5181-5183,5185-5198,5200-5203,5033,5205-5218,5110,5116,5036] State=UP MaxTime=240:00:00 AllowGroups=b1042 AllowAccounts=b1042 PriorityTier=2 PreemptMode=OFF
PartitionName=pulrseq Nodes=ALL State=UP MaxTime=48:00:00 AllowGroups=b1042 AllowAccounts=b1042 PriorityTier=2 PreemptMode=OFF
#PartitionName=pulrseq Nodes=qnode[5032,5045,5101,5008,5118-5119,5121-5127,5129,5130-5135,5137-5147,5149-5165,5167-5169,5171-5179,5102,5181-5183,5185-5198,5200-5203,5033,5205-5218,5110,5116,5036] State=UP MaxTime=48:00:00 AllowGroups=b1042 AllowAccounts=b1042 PriorityTier=2 PreemptMode=OFF
#
PartitionName=b1044 Nodes=ALL State=UP MaxTime=2-00:00:00 AllowGroups=b1044 AllowAccounts=b1044 PriorityTier=2 PreemptMode=OFF
#PartitionName=b1044 Nodes=qnode6063 State=UP AllowGroups=b1044 AllowAccounts=b1044 PriorityTier=2 PreemptMode=OFF
#
PartitionName=b1045 Nodes=ALL State=UP MaxTime=2-00:00:00 AllowGroups=b1045 AllowAccounts=b1045 PriorityTier=2 PreemptMode=OFF
#PartitionName=b1045 Nodes=qnode[6004,6006-6009,6018-6019,6022-6023,6056,6058-6061,6092] State=UP AllowGroups=b1045 AllowAccounts=b1045 PriorityTier=2 PreemptMode=OFF
#
PartitionName=b1048 Nodes=ALL State=UP MaxTime=2-00:00:00 AllowGroups=b1048 AllowAccounts=b1048 PriorityTier=2 PreemptMode=OFF
#PartitionName=b1048 Nodes=qnode[6017,6042-6046,6048-6051] State=UP AllowGroups=b1048 AllowAccounts=b1048 PriorityTier=2 PreemptMode=OFF
#
PartitionName=b1051 Nodes=ALL State=UP MaxTime=2-00:00:00 AllowGroups=b1051 AllowAccounts=b1051 PriorityTier=2 PreemptMode=OFF
#PartitionName=b1051 Nodes=qnode6003 State=UP AllowGroups=b1051 AllowAccounts=b1051 PriorityTier=2 PreemptMode=OFF
#
PartitionName=b1053 Nodes=ALL State=UP MaxTime=2-00:00:00 AllowGroups=b1053 AllowAccounts=b1053 PriorityTier=2 PreemptMode=OFF
#PartitionName=b1053 Nodes=qnode[6762-6765] State=UP AllowGroups=b1053 AllowAccounts=b1053 PriorityTier=2 PreemptMode=OFF
#
PartitionName=b1054 Nodes=ALL State=UP MaxTime=2-00:00:00 AllowGroups=b1054 AllowAccounts=b1054 PriorityTier=2 PreemptMode=OFF
#PartitionName=b1054 Nodes=qhimem[0010-0011] State=UP AllowGroups=b1054 AllowAccounts=b1054 PriorityTier=2 PreemptMode=OFF
#
PartitionName=b1065 Nodes=ALL State=UP MaxTime=2-00:00:00 AllowGroups=b1065 AllowAccounts=b1065 PriorityTier=2 PreemptMode=OFF
#PartitionName=b1065 Nodes=qnode[5004-5005] State=UP AllowGroups=b1065 AllowAccounts=b1065 PriorityTier=2 PreemptMode=OFF
#
PartitionName=b1068 Nodes=ALL State=UP MaxTime=2-00:00:00 AllowGroups=b1068 AllowAccounts=b1068 PriorityTier=2 PreemptMode=OFF
#PartitionName=b1068 Nodes=qnode[6752-6761] State=UP AllowGroups=b1068 AllowAccounts=b1068 PriorityTier=2 PreemptMode=OFF
#
PartitionName=b1075 Nodes=ALL State=UP MaxTime=2-00:00:00 AllowGroups=b1075 AllowAccounts=b1075 PriorityTier=2 PreemptMode=OFF
#PartitionName=b1075 Nodes=qnode[6749-6751,6766-6767] State=UP AllowGroups=b1075 AllowAccounts=b1075 PriorityTier=2 PreemptMode=OFF
#
PartitionName=b1081 Nodes=ALL State=UP MaxTime=2-00:00:00 AllowGroups=b1081 AllowAccounts=b1081 PriorityTier=2 PreemptMode=OFF
#PartitionName=b1081 Nodes=qnode[6846-6852],6872-6873,6094] State=UP AllowGroups=b1081 AllowAccounts=b1081 PriorityTier=2 PreemptMode=OFF
#
PartitionName=b1089 Nodes=ALL State=UP MaxTime=2-00:00:00 AllowGroups=b1089 AllowAccounts=b1089 PriorityTier=2 PreemptMode=OFF
#PartitionName=b1089 Nodes=qnode[6712-6717] State=UP AllowGroups=b1089 AllowAccounts=b1089 PriorityTier=2 PreemptMode=OFF
#
PartitionName=b1090 Nodes=ALL State=UP MaxTime=2-00:00:00 AllowGroups=b1090 AllowAccounts=b1090 PriorityTier=2 PreemptMode=OFF
#PartitionName=b1090 Nodes=qhimem[0012-0013] State=UP AllowGroups=b1090 AllowAccounts=b1090 PriorityTier=2 PreemptMode=OFF
#
#PartitionName=grail-std Nodes=qnode[6005,6085,6090-6091,6100,6103-6140,6142-6144] State=UP AllowGroups=b1094 AllowAccounts=b1091 PriorityTier=2 DefaultTime=2-00:00:00 PreemptMode=SUSPEND
#PartitionName=grail-ligo Nodes=qnode[6005,6085,6090-6091,6100,6103-6140,6142-6144] State=UP AllowGroups=b1094 AllowAccounts=b1091 PriorityTier=2 MaxTime=30-00:00:00 PreemptMode=SUSPEND
#PartitionName=grail-gpu Nodes=qgpu[6006-6015] State=UP AllowGroups=b1091 AllowAccounts=b1094 PriorityTier=2 PreemptMode=SUSPEND
#PartitionName=grail-gpu-preemptible Nodes=qgpu[6006-6015] State=UP AllowGroups=b1094 AllowAccounts=b1091 PriorityTier=1 MaxCPUsPerNode=24 PreemptMode=SUSPEND
#
#PartitionName=ciera-dev Nodes=qgpu6015 State=UP AllowGroups=b1092 AllowAccounts=b1095 PriorityTier=2 MaxTime=04:00:00 PreemptMode=SUSPEND
#PartitionName=ciera-std Nodes=qgpu6015 State=UP AllowGroups=b1092 AllowAccounts=b1095 PriorityTier=2 MaxTime=2-00:00:00 PreemptMode=SUSPEND
#PartitionName=ciera-preempt Nodes=qgpu6015 State=UP AllowGroups=b1092 AllowAccounts=b1095 PriorityTier=2 MaxTime=1-00:00:00 PreemptMode=SUSPEND
